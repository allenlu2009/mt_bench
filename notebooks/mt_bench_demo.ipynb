{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MT-bench Evaluation System Demo\n",
    "\n",
    "This notebook demonstrates the MT-bench evaluation system by cloning the repository and running the CLI demo.\n",
    "\n",
    "## Features\n",
    "- ðŸš€ Memory optimized for RTX 3060 (6GB VRAM)\n",
    "- âš¡ Fast evaluation with GPT-4.1-nano judge\n",
    "- ðŸ“Š Comprehensive analysis and visualization\n",
    "- ðŸ”§ Easy CLI interface\n",
    "- ðŸ§ª Well tested with unit tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\!rm -rf mt_bench  # Remove existing directory if it exists\n",
    "\!git clone https://github.com/allenlu2009/mt_bench.git  # Clone the repository\n",
    "%cd mt_bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support for GPU acceleration\n",
    "\!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Install MT-bench evaluation dependencies\n",
    "\!pip install -r requirements.txt\n",
    "\n",
    "# Optional: Install flash attention for memory optimization (may take a while)\n",
    "# \!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"Compute Capability: {torch.cuda.get_device_capability()}\")\n",
    "else:\n",
    "    print(\"Using CPU - evaluations will be slower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Info and CLI Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check MT-bench CLI help\n",
    "\!python -m src.cli --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available models with memory requirements\n",
    "\!python -m src.cli --list-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run CLI Demo\n",
    "\n",
    "The demo script shows CLI functionality without requiring OpenAI API keys or running full evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the CLI demo script (now we're in the mt_bench directory)\n",
    "\!python demo_cli.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Test (Optional - Requires OpenAI API Key)\n",
    "\n",
    "**Note**: The following cells require an OpenAI API key and will perform actual evaluations.\n",
    "Uncomment and run only if you have set up your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to set your OpenAI API key\n",
    "# import os\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-openai-api-key-here'\n",
    "# print(\"OpenAI API key set\!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with 1 question (fastest model)\n",
    "# \!python -m src.cli --models gpt2 --max-questions 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "## To run actual evaluations:\n",
    "\n",
    "1. **Set your OpenAI API key:**\n",
    "   ```python\n",
    "   import os\n",
    "   os.environ['OPENAI_API_KEY'] = 'your-api-key-here'\n",
    "   ```\n",
    "\n",
    "2. **Quick test (1 question, ~30 seconds):**\n",
    "   ```bash\n",
    "   \!python -m src.cli --models gpt2 --max-questions 1\n",
    "   ```\n",
    "\n",
    "3. **Small evaluation (5 questions, ~2-3 minutes):**\n",
    "   ```bash\n",
    "   \!python -m src.cli --models gpt2 llama-3.2-1b --max-questions 5\n",
    "   ```\n",
    "\n",
    "4. **Full evaluation (80 questions, 15-30 minutes):**\n",
    "   ```bash\n",
    "   \!python -m src.cli --models gpt2 llama-3.2-1b phi-3-mini\n",
    "   ```\n",
    "\n",
    "## Key Features:\n",
    "\n",
    "- âš¡ **Memory Optimized**: Flash Attention 2 for RTX 3060 (6GB VRAM)\n",
    "- ðŸ¤– **Multiple Models**: GPT-2, Llama, Phi-3, Qwen, Gemma support\n",
    "- ðŸ“Š **Rich Analysis**: Category breakdown, turn consistency, performance metrics\n",
    "- ðŸ”§ **Easy CLI**: Simple command-line interface with helpful options\n",
    "- ðŸ“ˆ **Detailed Results**: JSON, CSV, and summary formats\n",
    "- ðŸ§ª **Well Tested**: Comprehensive unit test suite\n",
    "\n",
    "## Documentation:\n",
    "\n",
    "- **Repository**: https://github.com/allenlu2009/mt_bench\n",
    "- **MT-bench Paper**: https://arxiv.org/abs/2306.05685\n",
    "- **Flash Attention**: https://arxiv.org/abs/2205.14135"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
EOF < /dev/null
