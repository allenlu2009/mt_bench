{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtbench_title"
   },
   "source": [
    "# MT-bench Evaluation System on Google Colab\n",
    "\n",
    "This notebook runs the MT-bench evaluation system on Google Colab with T4 GPU optimization.\n",
    "\n",
    "## Features:\n",
    "- Evaluates multiple language models on MT-bench\n",
    "- Uses GPT-4.1-nano as judge\n",
    "- Memory optimized for Colab's T4 GPU\n",
    "- Flash Attention 2 integration\n",
    "- Comprehensive results analysis\n",
    "\n",
    "## Requirements:\n",
    "- Google Colab with GPU runtime\n",
    "- OpenAI API key for judging\n",
    "- About 2-3 hours for full evaluation of 5 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. This will be very slow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch>=2.0.0 transformers>=4.36.0 accelerate>=0.24.1\n",
    "!pip install -q openai>=1.0.0 tenacity>=8.2.0 aiohttp>=3.8.0\n",
    "!pip install -q pandas tqdm psutil pyyaml\n",
    "!pip install -q bitsandbytes>=0.41.0\n",
    "\n",
    "# Try to install Flash Attention 2 (may take a few minutes)\n",
    "try:\n",
    "    !pip install -q flash-attn>=2.4.0 --no-build-isolation\n",
    "    print(\"‚úÖ Flash Attention 2 installed successfully\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Flash Attention 2 installation failed, will use standard attention\")\n",
    "\n",
    "print(\"\\n‚úÖ Package installation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_code"
   },
   "outputs": [],
   "source": [
    "# Download the MT-bench evaluation code\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "# For demo purposes, we'll create the code structure directly\n",
    "# In real usage, you would clone from your repository\n",
    "\n",
    "# Create directory structure\n",
    "os.makedirs('src/models', exist_ok=True)\n",
    "os.makedirs('src/evaluation', exist_ok=True)\n",
    "os.makedirs('src/utils', exist_ok=True)\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Code structure created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_section"
   },
   "source": [
    "## 2. Configuration and API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "api_key_setup"
   },
   "outputs": [],
   "source": [
    "# Set up OpenAI API key\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Get API key securely\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    api_key = getpass.getpass('Enter your OpenAI API key: ')\n",
    "    os.environ['OPENAI_API_KEY'] = api_key\n",
    "    print(\"‚úÖ API key set\")\n",
    "else:\n",
    "    print(\"‚úÖ API key already configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_selection"
   },
   "outputs": [],
   "source": [
    "# Select models to evaluate\n",
    "# These are optimized for Colab's T4 GPU (15GB memory)\n",
    "\n",
    "AVAILABLE_MODELS = {\n",
    "    'gpt2-large': {\n",
    "        'path': 'gpt2-large',\n",
    "        'memory_gb': 1.5,\n",
    "        'description': 'GPT-2 Large (774M params) - Fast baseline model'\n",
    "    },\n",
    "    'llama-3.2-1b': {\n",
    "        'path': 'meta-llama/Llama-3.2-1B-Instruct',\n",
    "        'memory_gb': 2.0,\n",
    "        'description': 'Llama 3.2 1B - Instruction-tuned model'\n",
    "    },\n",
    "    'phi-3-mini': {\n",
    "        'path': 'microsoft/Phi-3-mini-4k-instruct',\n",
    "        'memory_gb': 2.5,\n",
    "        'description': 'Phi-3 Mini - Efficient 3.8B parameter model'\n",
    "    },\n",
    "    'gemma-2b': {\n",
    "        'path': 'google/gemma-2b-it',\n",
    "        'memory_gb': 4.0,\n",
    "        'description': 'Gemma 2B IT - Instruction-tuned model'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Select models to evaluate (you can modify this list)\n",
    "MODELS_TO_EVALUATE = ['gpt2-large', 'llama-3.2-1b', 'phi-3-mini']\n",
    "\n",
    "# For quick testing, limit questions\n",
    "MAX_QUESTIONS = 10  # Set to None for full evaluation (80 questions)\n",
    "\n",
    "print(\"Selected models for evaluation:\")\n",
    "for model in MODELS_TO_EVALUATE:\n",
    "    info = AVAILABLE_MODELS[model]\n",
    "    print(f\"  - {model}: {info['description']} (~{info['memory_gb']}GB)\")\n",
    "\n",
    "print(f\"\\nQuestions per model: {MAX_QUESTIONS if MAX_QUESTIONS else 80}\")\n",
    "print(f\"Total evaluations: {len(MODELS_TO_EVALUATE) * (MAX_QUESTIONS if MAX_QUESTIONS else 80) * 2} (2 turns each)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "implementation_section"
   },
   "source": [
    "## 3. Core Implementation\n",
    "\n",
    "This section contains the main evaluation code. In a real deployment, this would be imported from modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "memory_utils"
   },
   "outputs": [],
   "source": [
    "# Memory monitoring utilities\n",
    "import torch\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "class MemoryMonitor:\n",
    "    def __init__(self, memory_limit_gb=15.0):\n",
    "        self.memory_limit_gb = memory_limit_gb\n",
    "        self.peak_memory = 0.0\n",
    "    \n",
    "    def get_gpu_memory_usage(self):\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.cuda.memory_allocated() / (1024**3)\n",
    "        return 0.0\n",
    "    \n",
    "    def cleanup_gpu_memory(self):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "    \n",
    "    def log_memory_usage(self, operation=\"\"):\n",
    "        gpu_memory = self.get_gpu_memory_usage()\n",
    "        system_memory = psutil.virtual_memory().percent\n",
    "        self.peak_memory = max(self.peak_memory, gpu_memory)\n",
    "        \n",
    "        print(f\"[{operation}] GPU: {gpu_memory:.2f}GB, System: {system_memory:.1f}%, Peak: {self.peak_memory:.2f}GB\")\n",
    "\n",
    "# Initialize memory monitor\n",
    "memory_monitor = MemoryMonitor()\n",
    "memory_monitor.log_memory_usage(\"Initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_manager"
   },
   "outputs": [],
   "source": [
    "# Model management with memory optimization\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import warnings\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, memory_monitor):\n",
    "        self.memory_monitor = memory_monitor\n",
    "        self.current_model = None\n",
    "        self.current_tokenizer = None\n",
    "        self.current_model_name = None\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    def cleanup_current_model(self):\n",
    "        if self.current_model is not None:\n",
    "            print(f\"Cleaning up model: {self.current_model_name}\")\n",
    "            del self.current_model\n",
    "            del self.current_tokenizer\n",
    "            self.current_model = None\n",
    "            self.current_tokenizer = None\n",
    "            self.current_model_name = None\n",
    "            self.memory_monitor.cleanup_gpu_memory()\n",
    "    \n",
    "    def load_model(self, model_name):\n",
    "        if self.current_model_name == model_name:\n",
    "            return self.current_model, self.current_tokenizer\n",
    "        \n",
    "        self.cleanup_current_model()\n",
    "        \n",
    "        model_info = AVAILABLE_MODELS[model_name]\n",
    "        model_path = model_info['path']\n",
    "        \n",
    "        print(f\"Loading model: {model_name} ({model_path})\")\n",
    "        self.memory_monitor.log_memory_usage(\"Before model loading\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\")\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Configure model loading\n",
    "        model_kwargs = {\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"device_map\": \"auto\",\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "        }\n",
    "        \n",
    "        # Try to use Flash Attention if available\n",
    "        try:\n",
    "            import flash_attn\n",
    "            model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "            print(\"Using Flash Attention 2\")\n",
    "        except ImportError:\n",
    "            print(\"Flash Attention 2 not available, using standard attention\")\n",
    "        \n",
    "        # Add trust_remote_code for certain models\n",
    "        if \"phi\" in model_name.lower() or \"qwen\" in model_name.lower():\n",
    "            model_kwargs[\"trust_remote_code\"] = True\n",
    "        \n",
    "        # Load model\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        self.current_model = model\n",
    "        self.current_tokenizer = tokenizer\n",
    "        self.current_model_name = model_name\n",
    "        \n",
    "        self.memory_monitor.log_memory_usage(\"After model loading\")\n",
    "        return model, tokenizer\n",
    "    \n",
    "    def generate_response(self, prompt, max_new_tokens=512):\n",
    "        if self.current_model is None:\n",
    "            raise RuntimeError(\"No model loaded\")\n",
    "        \n",
    "        inputs = self.current_tokenizer(\n",
    "            prompt, return_tensors=\"pt\", truncation=True, max_length=2048\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.current_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.current_tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "        \n",
    "        # Decode only the new tokens\n",
    "        input_length = inputs.input_ids.shape[1]\n",
    "        response_tokens = outputs[0][input_length:]\n",
    "        response = self.current_tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Initialize model manager\n",
    "model_manager = ModelManager(memory_monitor)\n",
    "print(\"‚úÖ Model manager initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_loader"
   },
   "outputs": [],
   "source": [
    "# MT-bench data loader\n",
    "import json\n",
    "import requests\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class MTBenchQuestion:\n",
    "    question_id: int\n",
    "    category: str\n",
    "    turns: List[str]\n",
    "\n",
    "def load_mtbench_questions():\n",
    "    \"\"\"Load MT-bench questions from official repository.\"\"\"\n",
    "    url = \"https://raw.githubusercontent.com/lm-sys/FastChat/main/fastchat/llm_judge/data/mt_bench/question.jsonl\"\n",
    "    \n",
    "    print(\"Downloading MT-bench questions...\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    questions = []\n",
    "    for line in response.text.strip().split('\\n'):\n",
    "        if line.strip():\n",
    "            data = json.loads(line)\n",
    "            questions.append(MTBenchQuestion(\n",
    "                question_id=data['question_id'],\n",
    "                category=data['category'],\n",
    "                turns=data['turns']\n",
    "            ))\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(questions)} MT-bench questions\")\n",
    "    \n",
    "    # Show category distribution\n",
    "    categories = {}\n",
    "    for q in questions:\n",
    "        categories[q.category] = categories.get(q.category, 0) + 1\n",
    "    \n",
    "    print(\"Categories:\")\n",
    "    for cat, count in sorted(categories.items()):\n",
    "        print(f\"  {cat}: {count} questions\")\n",
    "    \n",
    "    return questions\n",
    "\n",
    "# Load questions\n",
    "all_questions = load_mtbench_questions()\n",
    "\n",
    "# Limit questions for testing if specified\n",
    "if MAX_QUESTIONS:\n",
    "    questions = all_questions[:MAX_QUESTIONS]\n",
    "    print(f\"\\nüìù Using first {len(questions)} questions for testing\")\n",
    "else:\n",
    "    questions = all_questions\n",
    "    print(f\"\\nüìù Using all {len(questions)} questions for full evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "judge_client"
   },
   "outputs": [],
   "source": [
    "# OpenAI judge client\n",
    "import asyncio\n",
    "import time\n",
    "import re\n",
    "from openai import AsyncOpenAI\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class JudgeScore:\n",
    "    score: float\n",
    "    reasoning: str\n",
    "    question_id: int\n",
    "    turn: int\n",
    "    model_name: str\n",
    "\n",
    "class JudgeClient:\n",
    "    def __init__(self, api_key, model=\"gpt-4.1-nano\"):\n",
    "        self.client = AsyncOpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "        self.request_times = []\n",
    "    \n",
    "    async def rate_limit(self):\n",
    "        \"\"\"Simple rate limiting to stay under 10 req/sec.\"\"\"\n",
    "        current_time = time.time()\n",
    "        # Remove old timestamps\n",
    "        self.request_times = [t for t in self.request_times if current_time - t < 1.0]\n",
    "        \n",
    "        if len(self.request_times) >= 8:  # Be conservative\n",
    "            wait_time = 1.0 - (current_time - self.request_times[0])\n",
    "            if wait_time > 0:\n",
    "                await asyncio.sleep(wait_time)\n",
    "        \n",
    "        self.request_times.append(current_time)\n",
    "    \n",
    "    def parse_score(self, response_text):\n",
    "        \"\"\"Parse score from judge response.\"\"\"\n",
    "        # Look for [[score]] pattern\n",
    "        match = re.search(r'\\[\\[(\\d+(?:\\.\\d+)?)\\]\\]', response_text)\n",
    "        if match:\n",
    "            score = float(match.group(1))\n",
    "            reasoning = response_text[:match.start()].strip()\n",
    "            return score, reasoning\n",
    "        \n",
    "        # Fallback: look for any number 1-10\n",
    "        numbers = re.findall(r'\\b([1-9]|10)\\b', response_text)\n",
    "        if numbers:\n",
    "            return float(numbers[-1]), response_text\n",
    "        \n",
    "        return 0.0, \"Could not parse score\"\n",
    "    \n",
    "    async def judge_response(self, question, answer, question_id, turn, model_name):\n",
    "        \"\"\"Judge a single response.\"\"\"\n",
    "        prompt = f\"\"\"Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, please rate the response on a scale of 1 to 10 by strictly following this format: \"Rating: [[rating]]\", for example: \"Rating: [[8]]\".\n",
    "\n",
    "[Question]\n",
    "{question}\n",
    "\n",
    "[The Start of Assistant's Answer]\n",
    "{answer}\n",
    "[The End of Assistant's Answer]\"\"\"\n",
    "        \n",
    "        await self.rate_limit()\n",
    "        \n",
    "        try:\n",
    "            response = await self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert AI evaluator.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            \n",
    "            response_text = response.choices[0].message.content\n",
    "            score, reasoning = self.parse_score(response_text)\n",
    "            \n",
    "            return JudgeScore(\n",
    "                score=score,\n",
    "                reasoning=reasoning,\n",
    "                question_id=question_id,\n",
    "                turn=turn,\n",
    "                model_name=model_name\n",
    "            )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Judge error: {e}\")\n",
    "            return JudgeScore(\n",
    "                score=0.0,\n",
    "                reasoning=f\"Error: {e}\",\n",
    "                question_id=question_id,\n",
    "                turn=turn,\n",
    "                model_name=model_name\n",
    "            )\n",
    "\n",
    "# Initialize judge client\n",
    "judge_client = JudgeClient(os.environ['OPENAI_API_KEY'])\n",
    "print(\"‚úÖ Judge client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation_section"
   },
   "source": [
    "## 4. Run Evaluation\n",
    "\n",
    "This section runs the actual MT-bench evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prompt_templates"
   },
   "outputs": [],
   "source": [
    "# Prompt templates for different models\n",
    "PROMPT_TEMPLATES = {\n",
    "    'gpt2-large': '{instruction}',\n",
    "    'llama-3.2-1b': '<s>[INST] {instruction} [/INST]',\n",
    "    'phi-3-mini': '<|user|>\\n{instruction}<|end|>\\n<|assistant|>\\n',\n",
    "    'gemma-2b': '<start_of_turn>user\\n{instruction}<end_of_turn>\\n<start_of_turn>model\\n'\n",
    "}\n",
    "\n",
    "def format_prompt(instruction, model_name, conversation_history=None):\n",
    "    \"\"\"Format prompt for specific model.\"\"\"\n",
    "    template = PROMPT_TEMPLATES.get(model_name, '{instruction}')\n",
    "    \n",
    "    if conversation_history:\n",
    "        full_instruction = f\"{conversation_history}\\n\\nUser: {instruction}\\nAssistant:\"\n",
    "    else:\n",
    "        full_instruction = instruction\n",
    "    \n",
    "    return template.format(instruction=full_instruction)\n",
    "\n",
    "print(\"‚úÖ Prompt templates configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_evaluation"
   },
   "outputs": [],
   "source": [
    "# Main evaluation function\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "async def evaluate_model(model_name, questions):\n",
    "    \"\"\"Evaluate a single model on all questions.\"\"\"\n",
    "    print(f\"\\nüîÑ Starting evaluation of {model_name}\")\n",
    "    \n",
    "    # Load model\n",
    "    model, tokenizer = model_manager.load_model(model_name)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Progress bar\n",
    "    pbar = tqdm(questions, desc=f\"Evaluating {model_name}\")\n",
    "    \n",
    "    for question in pbar:\n",
    "        conversation_history = \"\"\n",
    "        \n",
    "        # Process both turns\n",
    "        for turn_num in [1, 2]:\n",
    "            turn_question = question.turns[turn_num - 1]\n",
    "            \n",
    "            # Format prompt\n",
    "            if turn_num == 1:\n",
    "                prompt = format_prompt(turn_question, model_name)\n",
    "            else:\n",
    "                prompt = format_prompt(turn_question, model_name, conversation_history)\n",
    "            \n",
    "            # Generate response\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                response = model_manager.generate_response(prompt)\n",
    "                generation_time = time.time() - start_time\n",
    "            except Exception as e:\n",
    "                print(f\"Generation error for Q{question.question_id} T{turn_num}: {e}\")\n",
    "                response = f\"Error: {e}\"\n",
    "                generation_time = 0.0\n",
    "            \n",
    "            # Judge response\n",
    "            judge_score = await judge_client.judge_response(\n",
    "                question=turn_question,\n",
    "                answer=response,\n",
    "                question_id=question.question_id,\n",
    "                turn=turn_num,\n",
    "                model_name=model_name\n",
    "            )\n",
    "            \n",
    "            # Store result\n",
    "            results.append({\n",
    "                'model_name': model_name,\n",
    "                'question_id': question.question_id,\n",
    "                'category': question.category,\n",
    "                'turn': turn_num,\n",
    "                'question': turn_question,\n",
    "                'response': response,\n",
    "                'score': judge_score.score,\n",
    "                'reasoning': judge_score.reasoning,\n",
    "                'generation_time': generation_time,\n",
    "                'memory_gb': memory_monitor.get_gpu_memory_usage()\n",
    "            })\n",
    "            \n",
    "            # Update conversation history for turn 2\n",
    "            if turn_num == 1:\n",
    "                conversation_history = f\"User: {turn_question}\\nAssistant: {response}\"\n",
    "        \n",
    "        # Update progress bar\n",
    "        current_scores = [r['score'] for r in results if r['model_name'] == model_name]\n",
    "        avg_score = np.mean(current_scores) if current_scores else 0.0\n",
    "        pbar.set_postfix({\n",
    "            'avg_score': f'{avg_score:.2f}',\n",
    "            'memory': f'{memory_monitor.get_gpu_memory_usage():.1f}GB'\n",
    "        })\n",
    "    \n",
    "    # Calculate final stats\n",
    "    model_scores = [r['score'] for r in results if r['model_name'] == model_name]\n",
    "    avg_score = np.mean(model_scores)\n",
    "    \n",
    "    print(f\"‚úÖ {model_name} completed - Average score: {avg_score:.2f}\")\n",
    "    memory_monitor.log_memory_usage(f\"Completed {model_name}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation for all models\n",
    "all_results = []\n",
    "\n",
    "for model_name in MODELS_TO_EVALUATE:\n",
    "    try:\n",
    "        model_results = await evaluate_model(model_name, questions)\n",
    "        all_results.extend(model_results)\n",
    "        \n",
    "        # Cleanup between models\n",
    "        model_manager.cleanup_current_model()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to evaluate {model_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nüéâ Evaluation completed! Total results: {len(all_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results_section"
   },
   "source": [
    "## 5. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze_results"
   },
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"üìä Results Summary:\")\n",
    "print(f\"Total evaluations: {len(df)}\")\n",
    "print(f\"Models evaluated: {df['model_name'].nunique()}\")\n",
    "print(f\"Questions evaluated: {df['question_id'].nunique()}\")\n",
    "print(f\"Categories covered: {df['category'].nunique()}\")\n",
    "\n",
    "# Overall scores by model\n",
    "print(\"\\nüèÜ Overall Rankings:\")\n",
    "model_scores = df.groupby('model_name')['score'].agg(['mean', 'std', 'count']).round(3)\n",
    "model_scores = model_scores.sort_values('mean', ascending=False)\n",
    "print(model_scores)\n",
    "\n",
    "# Category performance\n",
    "print(\"\\nüìà Performance by Category:\")\n",
    "category_scores = df.groupby(['model_name', 'category'])['score'].mean().unstack(fill_value=0).round(2)\n",
    "print(category_scores)\n",
    "\n",
    "# Turn comparison\n",
    "print(\"\\nüîÑ Performance by Turn:\")\n",
    "turn_scores = df.groupby(['model_name', 'turn'])['score'].mean().unstack(fill_value=0).round(2)\n",
    "print(turn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_results"
   },
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Overall scores by model\n",
    "model_means = df.groupby('model_name')['score'].mean().sort_values(ascending=True)\n",
    "axes[0, 0].barh(model_means.index, model_means.values, color='skyblue')\n",
    "axes[0, 0].set_title('Average MT-bench Scores by Model')\n",
    "axes[0, 0].set_xlabel('Average Score')\n",
    "for i, v in enumerate(model_means.values):\n",
    "    axes[0, 0].text(v + 0.1, i, f'{v:.2f}', va='center')\n",
    "\n",
    "# 2. Score distribution\n",
    "df.boxplot(column='score', by='model_name', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Score Distribution by Model')\n",
    "axes[0, 1].set_xlabel('Model')\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "\n",
    "# 3. Category heatmap\n",
    "category_pivot = df.pivot_table(values='score', index='model_name', columns='category', aggfunc='mean')\n",
    "sns.heatmap(category_pivot, annot=True, fmt='.2f', cmap='YlOrRd', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Performance Heatmap by Category')\n",
    "\n",
    "# 4. Turn comparison\n",
    "turn_data = df.groupby(['model_name', 'turn'])['score'].mean().unstack()\n",
    "turn_data.plot(kind='bar', ax=axes[1, 1], width=0.8)\n",
    "axes[1, 1].set_title('Turn 1 vs Turn 2 Performance')\n",
    "axes[1, 1].set_ylabel('Average Score')\n",
    "axes[1, 1].legend(['Turn 1', 'Turn 2'])\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance metrics\n",
    "print(\"\\n‚ö° Performance Metrics:\")\n",
    "perf_metrics = df.groupby('model_name').agg({\n",
    "    'generation_time': ['mean', 'sum'],\n",
    "    'memory_gb': 'max'\n",
    "}).round(3)\n",
    "perf_metrics.columns = ['Avg_Gen_Time', 'Total_Gen_Time', 'Peak_Memory_GB']\n",
    "print(perf_metrics)\n",
    "\n",
    "print(f\"\\nüî• Peak memory usage: {memory_monitor.peak_memory:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_results"
   },
   "outputs": [],
   "source": [
    "# Export results\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save detailed results\n",
    "csv_filename = f\"mtbench_results_{timestamp}.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"‚úÖ Detailed results saved to: {csv_filename}\")\n",
    "\n",
    "# Save summary\n",
    "summary_filename = f\"mtbench_summary_{timestamp}.txt\"\n",
    "with open(summary_filename, 'w') as f:\n",
    "    f.write(\"MT-BENCH EVALUATION SUMMARY\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Evaluation Date: {datetime.datetime.now()}\\n\")\n",
    "    f.write(f\"Models Evaluated: {', '.join(MODELS_TO_EVALUATE)}\\n\")\n",
    "    f.write(f\"Questions per Model: {len(questions)}\\n\")\n",
    "    f.write(f\"Total Evaluations: {len(df)}\\n\\n\")\n",
    "    \n",
    "    f.write(\"OVERALL RANKINGS:\\n\")\n",
    "    for i, (model, score) in enumerate(model_scores['mean'].items(), 1):\n",
    "        f.write(f\"{i}. {model}: {score:.3f}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nPeak Memory Usage: {memory_monitor.peak_memory:.2f}GB\\n\")\n",
    "    f.write(f\"Total Evaluation Time: {df['generation_time'].sum():.1f}s\\n\")\n",
    "\n",
    "print(f\"‚úÖ Summary report saved to: {summary_filename}\")\n",
    "\n",
    "# Download files (in Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(csv_filename)\n",
    "    files.download(summary_filename)\n",
    "    print(\"üì• Files ready for download!\")\n",
    "except ImportError:\n",
    "    print(\"üíæ Files saved locally (not in Colab environment)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion_section"
   },
   "source": [
    "## 6. Conclusion and Next Steps\n",
    "\n",
    "This notebook has successfully evaluated multiple language models on the MT-bench benchmark using GPT-4.1-nano as a judge.\n",
    "\n",
    "### Key Results:\n",
    "- **Models Evaluated**: Various sizes from GPT-2 Large to Gemma 2B\n",
    "- **Memory Optimization**: Used Flash Attention 2 and fp16 precision\n",
    "- **Comprehensive Analysis**: Category-wise and turn-wise performance\n",
    "\n",
    "### Next Steps:\n",
    "1. **Scale Up**: Run full evaluation with all 80 questions\n",
    "2. **More Models**: Evaluate larger models with quantization\n",
    "3. **Custom Categories**: Focus on specific domains of interest\n",
    "4. **Fine-tuning**: Use results to guide model improvement\n",
    "\n",
    "### Resources:\n",
    "- [MT-bench Paper](https://arxiv.org/abs/2306.05685)\n",
    "- [FastChat Repository](https://github.com/lm-sys/FastChat)\n",
    "- [Flash Attention](https://github.com/Dao-AILab/flash-attention)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}