"""Supported model registry."""

from typing import Dict

from .model_types import ModelConfig


AVAILABLE_MODELS: Dict[str, ModelConfig] = {
    "gpt2": ModelConfig(
        model_path="gpt2",
        model_family="gpt2",
        prompt_template="{instruction}",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=0.5,
        requires_system_prompt=False,
    ),
    "gpt2-large": ModelConfig(
        model_path="gpt2-large",
        model_family="gpt2",
        prompt_template="{instruction}",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=1.5,
        requires_system_prompt=False,
        quantization_format="FP16",
    ),
    "llama-3.2-1b": ModelConfig(
        model_path="meta-llama/Llama-3.2-1B-Instruct",
        model_family="llama",
        prompt_template="<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=2.3,
        requires_system_prompt=False,
        chat_template_name="llama",
        quantization_format="FP16",
    ),
    "llama-3.2-3b": ModelConfig(
        model_path="meta-llama/Llama-3.2-3B-Instruct",
        model_family="llama",
        prompt_template="<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=6.0,
        requires_system_prompt=False,
        chat_template_name="llama",
        quantization_format="FP16",
    ),
    "phi-3-mini": ModelConfig(
        model_path="microsoft/Phi-3-mini-4k-instruct",
        model_family="phi",
        prompt_template="<|user|>\n{instruction}<|end|>\n<|assistant|>\n",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=7.1,
        requires_system_prompt=False,
        chat_template_name="phi",
        quantization_format="FP16",
    ),
    "qwen2.5-3b": ModelConfig(
        model_path="Qwen/Qwen2.5-3B-Instruct",
        model_family="qwen",
        prompt_template="<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=5.9,
        requires_system_prompt=False,
        chat_template_name="qwen",
        quantization_format="FP16",
    ),
    "gemma-2b": ModelConfig(
        model_path="google/gemma-2b-it",
        model_family="gemma",
        prompt_template="<start_of_turn>user\n{instruction}<end_of_turn>\n<start_of_turn>model\n",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=4.0,
        requires_system_prompt=False,
        chat_template_name="gemma",
    ),
    "gpt2-large-conversational": ModelConfig(
        model_path="Locutusque/gpt2-large-conversational-retrain",
        model_family="gpt2",
        prompt_template="<|USER|> {instruction} <|ASSISTANT|> ",
        max_new_tokens=256,
        temperature=0.3,
        top_p=0.7,
        estimated_memory_gb=1.5,
        requires_system_prompt=False,
        chat_template_name="gpt2_conversational",
    ),
    "dialogpt-large": ModelConfig(
        model_path="microsoft/DialoGPT-large",
        model_family="gpt2",
        prompt_template="{instruction}",
        max_new_tokens=256,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=1.5,
        requires_system_prompt=False,
        chat_template_name="dialogpt",
    ),
    "gemma3-270m": ModelConfig(
        model_path="google/gemma-3-270m-it",
        model_family="gemma3",
        prompt_template="<start_of_turn>user\n{instruction}<end_of_turn>\n<start_of_turn>model\n",
        max_new_tokens=256,
        temperature=0.0,
        top_p=1.0,
        estimated_memory_gb=0.6,
        requires_system_prompt=False,
        chat_template_name="gemma",
        quantization_format="BF16",
    ),
    "gemma3-1b": ModelConfig(
        model_path="google/gemma-3-1b-it",
        model_family="gemma3",
        prompt_template="<start_of_turn>user\n{instruction}<end_of_turn>\n<start_of_turn>model\n",
        max_new_tokens=256,
        temperature=0.0,
        top_p=1.0,
        estimated_memory_gb=2.2,
        requires_system_prompt=False,
        chat_template_name="gemma",
        quantization_format="BF16",
    ),
    "gemma3-4b": ModelConfig(
        model_path="google/gemma-3-4b-it",
        model_family="gemma3",
        prompt_template="<start_of_turn>user\n{instruction}<end_of_turn>\n<start_of_turn>model\n",
        max_new_tokens=256,
        temperature=0.0,
        top_p=1.0,
        estimated_memory_gb=8.8,
        requires_system_prompt=False,
        chat_template_name="gemma",
        quantization_format="BF16",
    ),
    "qwen3-4b-instruct-fp8": ModelConfig(
        model_path="Qwen/Qwen3-4B-Instruct-2507-FP8",
        model_family="qwen",
        prompt_template="<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=5.0,
        requires_system_prompt=False,
        chat_template_name="qwen",
        quantization_format="FP8",
    ),
    "qwen3-4b-instruct": ModelConfig(
        model_path="Qwen/Qwen3-4B-Instruct-2507",
        model_family="qwen",
        prompt_template="<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=8.8,
        requires_system_prompt=False,
        chat_template_name="qwen",
        quantization_format="BF16",
    ),
    "qwen3-4b-instruct-4bit": ModelConfig(
        model_path="Qwen/Qwen3-4B-Instruct-2507",
        model_family="qwen",
        prompt_template="<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=3.5,
        requires_system_prompt=False,
        chat_template_name="qwen",
        quantization_format="INT4",
        load_in_4bit=True,
    ),
    "qwen2.5-32b": ModelConfig(
        model_path="Qwen/Qwen2.5-32B-Instruct",
        model_family="qwen",
        prompt_template="<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=64.0,
        requires_system_prompt=False,
        chat_template_name="qwen",
        quantization_format="BF16",
    ),
    "qwen3-32b": ModelConfig(
        model_path="Qwen/Qwen3-32B-Instruct-2507",
        model_family="qwen",
        prompt_template="<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=64.0,
        requires_system_prompt=False,
        chat_template_name="qwen",
        quantization_format="BF16",
    ),
    "qwen3-0.6b": ModelConfig(
        model_path="Qwen/Qwen3-0.6B",
        model_family="qwen",
        prompt_template="<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=1.3,
        requires_system_prompt=False,
        chat_template_name="qwen",
        quantization_format="BF16",
    ),
    "qwen3-8b": ModelConfig(
        model_path="Qwen/Qwen3-8B",
        model_family="qwen",
        prompt_template="<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=17.5,
        requires_system_prompt=False,
        chat_template_name="qwen",
        quantization_format="BF16",
    ),
    "qwen3-8b-instruct": ModelConfig(
        model_path="Qwen/Qwen3-8B-Instruct",
        model_family="qwen",
        prompt_template="<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=17.5,
        requires_system_prompt=False,
        chat_template_name="qwen",
        quantization_format="BF16",
    ),
    "qwen3-8b-fp8": ModelConfig(
        model_path="Qwen/Qwen3-8B-FP8",
        model_family="qwen",
        prompt_template="<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=9.0,
        requires_system_prompt=False,
        chat_template_name="qwen",
        quantization_format="FP8",
    ),
    "qwen3-30b-a3b": ModelConfig(
        model_path="Qwen/Qwen3-30B-A3B",
        model_family="qwen",
        prompt_template="<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=61.0,
        requires_system_prompt=False,
        chat_template_name="qwen",
        quantization_format="BF16",
    ),
    "qwen3-30b-a3b-fp8": ModelConfig(
        model_path="Qwen/Qwen3-30B-A3B-FP8",
        model_family="qwen",
        prompt_template="<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=32.0,
        requires_system_prompt=False,
        chat_template_name="qwen",
        quantization_format="FP8",
    ),
    "qwen3-32b-fp8": ModelConfig(
        model_path="Qwen/Qwen3-32B-FP8",
        model_family="qwen",
        prompt_template="<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=34.0,
        requires_system_prompt=False,
        chat_template_name="qwen",
        quantization_format="FP8",
    ),
    "qwen2-57b-a14b-instruct": ModelConfig(
        model_path="Qwen/Qwen2-57B-A14B-Instruct",
        model_family="qwen",
        prompt_template="<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n",
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        estimated_memory_gb=114.0,
        requires_system_prompt=False,
        chat_template_name="qwen",
        quantization_format="BF16",
    ),
}


MODEL_ALIASES: Dict[str, str] = {
    # Backward compatibility with prior mt_bench names
    "qwen3-4b": "qwen3-4b-instruct",
    "qwen3-4b-fp8": "qwen3-4b-instruct-fp8",
    # Perplexity-style aliases
    "llama3.2-1b": "llama-3.2-1b",
    "llama3.2-3b": "llama-3.2-3b",
    "phi3-mini-4k": "phi-3-mini",
    "qwen3-4b-instruct-2507": "qwen3-4b-instruct",
    "qwen3-4b-instruct-2507-fp8": "qwen3-4b-instruct-fp8",
}
